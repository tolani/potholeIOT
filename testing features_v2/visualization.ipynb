{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.fftpack\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>accx</th>\n",
       "      <th>accy</th>\n",
       "      <th>accz</th>\n",
       "      <th>gyrx</th>\n",
       "      <th>gyry</th>\n",
       "      <th>gyrz</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>152178985702</td>\n",
       "      <td>0.152901</td>\n",
       "      <td>-0.657606</td>\n",
       "      <td>9.569601</td>\n",
       "      <td>-0.002742</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>-0.005377</td>\n",
       "      <td>77.643354</td>\n",
       "      <td>12.87931</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152178985720</td>\n",
       "      <td>-0.444470</td>\n",
       "      <td>-0.501953</td>\n",
       "      <td>9.585513</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>-0.007851</td>\n",
       "      <td>-0.005371</td>\n",
       "      <td>77.643354</td>\n",
       "      <td>12.87931</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152178985742</td>\n",
       "      <td>-0.195615</td>\n",
       "      <td>-0.610109</td>\n",
       "      <td>10.012871</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>-0.004395</td>\n",
       "      <td>77.643354</td>\n",
       "      <td>12.87931</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152178985764</td>\n",
       "      <td>-0.310112</td>\n",
       "      <td>-0.708696</td>\n",
       "      <td>9.713529</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>-0.009312</td>\n",
       "      <td>-0.004761</td>\n",
       "      <td>77.643354</td>\n",
       "      <td>12.87931</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152178985784</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.345822</td>\n",
       "      <td>10.250119</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>-0.003700</td>\n",
       "      <td>-0.004639</td>\n",
       "      <td>77.643354</td>\n",
       "      <td>12.87931</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp      accx      accy       accz      gyrx      gyry      gyrz  \\\n",
       "0  152178985702  0.152901 -0.657606   9.569601 -0.002742  0.008070 -0.005377   \n",
       "1  152178985720 -0.444470 -0.501953   9.585513  0.000966 -0.007851 -0.005371   \n",
       "2  152178985742 -0.195615 -0.610109  10.012871  0.000354 -0.009804 -0.004395   \n",
       "3  152178985764 -0.310112 -0.708696   9.713529  0.000969 -0.009312 -0.004761   \n",
       "4  152178985784 -0.400679 -0.345822  10.250119 -0.000861 -0.003700 -0.004639   \n",
       "\n",
       "   longitude  latitude  speed  \n",
       "0  77.643354  12.87931    0.0  \n",
       "1  77.643354  12.87931    0.0  \n",
       "2  77.643354  12.87931    0.0  \n",
       "3  77.643354  12.87931    0.0  \n",
       "4  77.643354  12.87931    0.0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('./sensor_data/sensorData_23-03-2018_15217898569-12_55.txt')\n",
    "df2 = pd.read_csv('./sensor_data/sensorData_23-03-2018_15217899985-1_02.txt')\n",
    "df3 = pd.read_csv('./sensor_data/sensorData_23-03-2018_15217903881-1_06.txt')\n",
    "df4 = pd.read_csv('./sensor_data/sensorData_23-03-2018_15217906095-1_09.txt')\n",
    "df5 = pd.read_csv('./sensor_data/sensorData_23-03-2018_15217913809-1_24.txt')\n",
    "df6 = pd.read_csv('./sensor_data/sensorData_23-03-2018_15217883948-12_31.txt')\n",
    "df7 = pd.read_csv('./sensor_data/sensorData_23-03-2018_15217908058-1_13.txt')\n",
    "df8 = pd.read_csv('./sensor_data_old/sensorData_19-03-2018_15214527501_15-18.txt')\n",
    "df9 = pd.read_csv('./sensor_data_old/sensorData_19-03-2018_15214537070_15-34.txt')\n",
    "df10 = pd.read_csv('./sensor_data_old/sensorData_19-03-2018_15214540891_15-41.txt')\n",
    "df11 = pd.read_csv('./sensor_data_old/sensorData_19-03-2018_15214551353_15-58.txt')\n",
    "df12 = pd.read_csv('./sensor_data_old/sensorData_19-03-2018_15214560034_16-13.txt')\n",
    "df13 = pd.read_csv('./sensor_data_new/sensorData_04-04-2018_15228184860-10_53.txt')\n",
    "df14 = pd.read_csv('./sensor_data_new/sensorData_04-04-2018_15228194973-10_58.txt')\n",
    "df15 = pd.read_csv('./sensor_data_new/sensorData_04-04-2018_15228197814-11_02.txt')\n",
    "df16 = pd.read_csv('./sensor_data_new/sensorData_04-04-2018_15228200158-11_06.txt')\n",
    "df17 = pd.read_csv('./sensor_data_new/sensorData_04-04-2018_15228202727-11_14.txt')\n",
    "df18 = pd.read_csv('./sensor_data_new/sensorData_04-04-2018_15228207467-11_36.txt')\n",
    "df19 = pd.read_csv('./sensor_data_new/sensorData_04-04-2018_15228271593-1_24.txt')\n",
    "df20 = pd.read_csv('./sensor_data_new/sensorData_04-04-2018_15228285553-1_47.txt')\n",
    "\n",
    "\n",
    "# dataframes for pothole annotated files\n",
    "\n",
    "pdf1 = pd.read_csv('./potholes/sensorData_23-03-2018_15217898607-12_55.txt')\n",
    "pdf2 = pd.read_csv('./potholes/sensorData_23-03-2018_15217899997-1_02.txt')\n",
    "pdf3 = pd.read_csv('./potholes/sensorData_23-03-2018_15217903905-1_06.txt')\n",
    "pdf4 = pd.read_csv('./potholes/sensorData_23-03-2018_15217906130-1_09.txt')\n",
    "pdf5 = pd.read_csv('./potholes/sensorData_23-03-2018_15217914003-1_24.txt')\n",
    "pdf6 = pd.read_csv('./potholes/sensorData_23-03-2018_15217883957-12_31.txt')\n",
    "pdf7 = pd.read_csv('./potholes/sensorData_23-03-2018_15217908081-1_13.txt')\n",
    "pdf8 = pd.read_csv('./potholes_old/sensorData_19-03-2018_15214527532_15-18.txt')\n",
    "pdf9 = pd.read_csv('./potholes_old/sensorData_19-03-2018_15214537329_15-34.txt')\n",
    "pdf10 = pd.read_csv('./potholes_old/sensorData_19-03-2018_15214540946_15-41.txt')\n",
    "pdf11 = pd.read_csv('./potholes_old/sensorData_19-03-2018_15214551395_15-58.txt')\n",
    "pdf12 = pd.read_csv('./potholes_old/sensorData_19-03-2018_15214560096_16-13.txt')\n",
    "pdf13 = pd.read_csv('./potholes_new/sensorData_04-04-2018_15228184880-10_53.txt')\n",
    "pdf14 = pd.read_csv('./potholes_new/sensorData_04-04-2018_15228194989-10_58.txt')\n",
    "pdf15 = pd.read_csv('./potholes_new/sensorData_04-04-2018_15228197815-11_02.txt')\n",
    "pdf16 = pd.read_csv('./potholes_new/sensorData_04-04-2018_15228200151-11_06.txt')\n",
    "pdf17 = pd.read_csv('./potholes_new/sensorData_04-04-2018_15228202691-11_14.txt')\n",
    "pdf18 = pd.read_csv('./potholes_new/sensorData_04-04-2018_15228207479-11_36.txt')\n",
    "pdf19 = pd.read_csv('./potholes_new/sensorData_04-04-2018_15228271641-1_24.txt')\n",
    "pdf20 = pd.read_csv('./potholes_new/sensorData_04-04-2018_15228285574-1_47.txt')\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_start</th>\n",
       "      <th>ts_end</th>\n",
       "      <th>mean_ax</th>\n",
       "      <th>mean_ay</th>\n",
       "      <th>mean_az</th>\n",
       "      <th>mean_gx</th>\n",
       "      <th>mean_gy</th>\n",
       "      <th>mean_gz</th>\n",
       "      <th>sd_ax</th>\n",
       "      <th>sd_ay</th>\n",
       "      <th>...</th>\n",
       "      <th>min_gx</th>\n",
       "      <th>min_gy</th>\n",
       "      <th>min_gz</th>\n",
       "      <th>max_ax</th>\n",
       "      <th>max_ay</th>\n",
       "      <th>max_az</th>\n",
       "      <th>max_gx</th>\n",
       "      <th>max_gy</th>\n",
       "      <th>max_gz</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ts_start, ts_end, mean_ax, mean_ay, mean_az, mean_gx, mean_gy, mean_gz, sd_ax, sd_ay, sd_az, sd_gx, sd_gy, sd_gz, min_ax, min_ay, min_az, min_gx, min_gy, min_gz, max_ax, max_ay, max_az, max_gx, max_gy, max_gz, label]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main = pd.read_csv('./features.txt')\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is to calcualte features by aggregating 10 data pts\n",
    "df_index =0\n",
    "\n",
    "for j in range(0,19):\n",
    "    \n",
    "    if(df_index == 0):\n",
    "        df = df1\n",
    "        pdf = pdf1\n",
    "    elif(df_index == 1):\n",
    "        df = df2\n",
    "        pdf = pdf2\n",
    "    elif(df_index == 2):\n",
    "        df = df3\n",
    "        pdf = pdf3\n",
    "    elif(df_index == 3):\n",
    "        df = df4\n",
    "        pdf = pdf4\n",
    "    elif(df_index == 4):\n",
    "        df = df5\n",
    "        pdf = pdf5\n",
    "    elif(df_index == 5):\n",
    "        df = df6\n",
    "        pdf = pdf6\n",
    "    elif(df_index == 6):\n",
    "        df = df7\n",
    "        pdf = pdf7\n",
    "    elif(df_index == 7):\n",
    "        df = df8\n",
    "        pdf = pdf8\n",
    "    elif(df_index == 8):\n",
    "        df = df9\n",
    "        pdf = pdf9\n",
    "    \n",
    "    elif(df_index == 9):\n",
    "        df = df11\n",
    "        pdf = pdf11\n",
    "    elif(df_index == 10):\n",
    "        df = df12\n",
    "        pdf = pdf12\n",
    "    elif(df_index == 11):\n",
    "        df = df13\n",
    "        pdf = pdf13\n",
    "    elif(df_index == 12):\n",
    "        df = df14\n",
    "        pdf = pdf14\n",
    "    elif(df_index == 13):\n",
    "        df = df15\n",
    "        pdf = pdf15\n",
    "    elif(df_index == 14):\n",
    "        df = df16\n",
    "        pdf = pdf16\n",
    "    elif(df_index == 15):\n",
    "        df = df17\n",
    "        pdf = pdf17\n",
    "    elif(df_index == 16):\n",
    "        df = df18\n",
    "        pdf = pdf18\n",
    "    elif(df_index == 17):\n",
    "        df = df19\n",
    "        pdf = pdf19\n",
    "    elif(df_index == 18):\n",
    "        df = df20\n",
    "        pdf = pdf20\n",
    "    else:\n",
    "        df = df10\n",
    "        pdf = pdf10\n",
    "    \n",
    "    df_index += 1\n",
    "    count = 0\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(1,len(df),10):    # step size is 10 means aggregrating 10 data pts means 1 second data\n",
    "        if(i+9 >= len(df)):\n",
    "            break\n",
    "        #print(i)\n",
    "        dt = df[i-1:i+10]      # chunking the given dataframe into smaller dataframe containing 10 pts\n",
    "        start = dt.timestamp[i-1]\n",
    "        end = dt.timestamp[i+9]\n",
    "        a = dt.mean()      # will give an array of mean of columns of dt\n",
    "        mean_ax = a[1]\n",
    "        mean_ay = a[2]\n",
    "        mean_az = a[3]\n",
    "#         mean_gx = a[4]\n",
    "#         mean_gy = a[5]\n",
    "#         mean_gz = a[6]\n",
    "        a = dt.min()\n",
    "        min_ax = a[1]\n",
    "        min_ay = a[2]\n",
    "        min_az = a[3]\n",
    "#         min_gx = a[4]\n",
    "#         min_gy = a[5]\n",
    "#         min_gz = a[6]\n",
    "        a = dt.max()\n",
    "        max_ax = a[1]\n",
    "        max_ay = a[2]\n",
    "        max_az = a[3]\n",
    "#         max_gx = a[4]\n",
    "#         max_gy = a[5]\n",
    "#         max_gz = a[6]\n",
    "        a = dt.std()\n",
    "        sd_ax = a[1]\n",
    "        sd_ay = a[2]\n",
    "        sd_az = a[3]\n",
    "#         sd_gx = a[4]\n",
    "#         sd_gy = a[5]\n",
    "#         sd_gz = a[6]\n",
    "        a = dt.var()\n",
    "        var_ax = a[1]\n",
    "        var_ay = a[2]\n",
    "        var_az = a[3]\n",
    "        \n",
    "        #adding max-min\n",
    "        mm_x = max_ax - min_ax\n",
    "        mm_y = max_ay - min_ay\n",
    "        mm_z = max_az - min_az\n",
    "        \n",
    "\n",
    "        #taking gradients\n",
    "        arx = dt['accx']\n",
    "        ary = dt['accy']\n",
    "        arz = dt['accz']\n",
    "        tm = dt['timestamp']\n",
    "        dx = np.gradient(arx, tm).max()\n",
    "        dy = np.gradient(ary, tm).max()\n",
    "        dz = np.gradient(arz, tm).max()\n",
    "        \n",
    "        #taking fourier transforms\n",
    "        ft = scipy.fftpack.fft(dt)\n",
    "        \n",
    "        fft_ax = ft[1].max().imag\n",
    "        fft_ay = ft[2].max().imag\n",
    "        fft_az = ft[3].max().imag\n",
    "        \n",
    "        #getting spectral energy\n",
    "        sp_ax = np.mean(np.square(ft[1].real) + np.square(ft[1].imag))\n",
    "        sp_ay = np.mean(np.square(ft[2].real) + np.square(ft[2].imag))\n",
    "        sp_az = np.mean(np.square(ft[3].real) + np.square(ft[3].imag))\n",
    "        \n",
    "        # adding label\n",
    "        if(k >= len(pdf)):\n",
    "            break        \n",
    "        \n",
    "        if(pdf['timestamp'][k] > start and pdf['timestamp'][k] <= end ):\n",
    "            label = 1\n",
    "            k = k + 1\n",
    "            #print(\"haha\")\n",
    "            \n",
    "            if(k >= len(pdf)):\n",
    "                break\n",
    "            while(pdf['timestamp'][k] > start and pdf['timestamp'][k] <= end):\n",
    "                k = k + 1\n",
    "                if(k >= len(pdf)):\n",
    "                        break\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        df_temp = pd.DataFrame([[mean_ax, mean_ay, mean_az, \n",
    "                                 sd_ax, sd_ay, sd_az,min_ax, min_ay,\n",
    "                                 min_az, max_ax, max_ay, max_az, var_ax, \n",
    "                                 var_ay, var_az ,mm_x, mm_y, mm_z, dx, dy, dz,fft_ax,\n",
    "                                 fft_ay, fft_az,sp_ax, sp_ay, sp_az, start, end, label]], \n",
    "\n",
    "                              columns = ('mean_ax','mean_ay','mean_az',\n",
    "                                         'sd_ax','sd_ay','sd_az','min_ax','min_ay',\n",
    "                                         'min_az','max_ax','max_ay','max_az', 'var_ax', 'var_ay',\n",
    "                                         'var_az','mm_x', 'mm_y','mm_z','dx', 'dy','dz','fft_ax',\n",
    "                                         'fft_ay','fft_az','sp_ax', 'sp_ay', 'sp_az',\n",
    "                                         'ts_start','ts_end','label'))\n",
    "        \n",
    "        df_main = df_main.append(df_temp)\n",
    "        #count = count + 1\n",
    "        #i = i+20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_temp = pd.DataFrame([[start, end, mean_ax, mean_ay, mean_az, mean_gx,mean_gy, mean_gz, \n",
    "                               sd_ax, sd_ay, sd_az, sd_gx, sd_gy, sd_gz,min_ax, min_ay,\n",
    "                               min_az, min_gx, min_gy, min_gz, max_ax, max_ay, max_az, max_gx, max_gy, max_gz , label]], \n",
    "\n",
    "                              columns = ('ts_start','ts_end','mean_ax','mean_ay','mean_az','mean_gx','mean_gy',\n",
    "                                         'mean_gz','sd_ax','sd_ay','sd_az','sd_gx','sd_gy','sd_gz','min_ax','min_ay',\n",
    "                                         'min_az','min_gx','min_gy','min_gz','max_ax','max_ay','max_az','max_gx',\n",
    "                                         'max_gy','max_gz', 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_main = df_main.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>dz</th>\n",
       "      <th>fft_ax</th>\n",
       "      <th>fft_ay</th>\n",
       "      <th>fft_az</th>\n",
       "      <th>label</th>\n",
       "      <th>max_ax</th>\n",
       "      <th>max_ay</th>\n",
       "      <th>max_az</th>\n",
       "      <th>...</th>\n",
       "      <th>sd_ay</th>\n",
       "      <th>sd_az</th>\n",
       "      <th>sp_ax</th>\n",
       "      <th>sp_ay</th>\n",
       "      <th>sp_az</th>\n",
       "      <th>ts_end</th>\n",
       "      <th>ts_start</th>\n",
       "      <th>var_ax</th>\n",
       "      <th>var_ay</th>\n",
       "      <th>var_az</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009452</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.152901</td>\n",
       "      <td>-0.226779</td>\n",
       "      <td>10.250119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162682</td>\n",
       "      <td>0.286165</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>152178985912</td>\n",
       "      <td>152178985702</td>\n",
       "      <td>0.058883</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.081890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023832</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.016422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.041873</td>\n",
       "      <td>-0.205003</td>\n",
       "      <td>10.182401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158947</td>\n",
       "      <td>0.201366</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>152178986124</td>\n",
       "      <td>152178985912</td>\n",
       "      <td>0.064644</td>\n",
       "      <td>0.025264</td>\n",
       "      <td>0.040548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.014844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383211</td>\n",
       "      <td>0.875952</td>\n",
       "      <td>10.347148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514140</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>152178986337</td>\n",
       "      <td>152178986124</td>\n",
       "      <td>0.077985</td>\n",
       "      <td>0.264340</td>\n",
       "      <td>0.049907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015739</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.085185</td>\n",
       "      <td>1.039981</td>\n",
       "      <td>10.145551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213451</td>\n",
       "      <td>0.271166</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>152178986549</td>\n",
       "      <td>152178986337</td>\n",
       "      <td>0.055386</td>\n",
       "      <td>0.045561</td>\n",
       "      <td>0.073531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>0.036462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108875</td>\n",
       "      <td>0.941278</td>\n",
       "      <td>10.244975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272872</td>\n",
       "      <td>0.344815</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>152178986762</td>\n",
       "      <td>152178986549</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.074459</td>\n",
       "      <td>0.118898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dx        dy        dz  fft_ax  fft_ay  fft_az label    max_ax  \\\n",
       "0  0.009452  0.010545  0.018279     0.0     0.0     0.0     0  0.152901   \n",
       "0  0.023832  0.009768  0.016422     0.0     0.0     0.0     0 -0.041873   \n",
       "0  0.012127  0.025754  0.014844     0.0     0.0     0.0     0  0.383211   \n",
       "0  0.015739  0.013850  0.008753     0.0     0.0     0.0     0  0.085185   \n",
       "0  0.003754  0.013616  0.036462     0.0     0.0     0.0     0 -0.108875   \n",
       "\n",
       "     max_ay     max_az    ...        sd_ay     sd_az         sp_ax  \\\n",
       "0 -0.226779  10.250119    ...     0.162682  0.286165  2.315844e+22   \n",
       "0 -0.205003  10.182401    ...     0.158947  0.201366  2.315844e+22   \n",
       "0  0.875952  10.347148    ...     0.514140  0.223400  2.315844e+22   \n",
       "0  1.039981  10.145551    ...     0.213451  0.271166  2.315844e+22   \n",
       "0  0.941278  10.244975    ...     0.272872  0.344815  2.315844e+22   \n",
       "\n",
       "          sp_ay         sp_az        ts_end      ts_start    var_ax    var_ay  \\\n",
       "0  2.315844e+22  2.315844e+22  152178985912  152178985702  0.058883  0.026466   \n",
       "0  2.315844e+22  2.315844e+22  152178986124  152178985912  0.064644  0.025264   \n",
       "0  2.315844e+22  2.315844e+22  152178986337  152178986124  0.077985  0.264340   \n",
       "0  2.315844e+22  2.315844e+22  152178986549  152178986337  0.055386  0.045561   \n",
       "0  2.315844e+22  2.315844e+22  152178986762  152178986549  0.023541  0.074459   \n",
       "\n",
       "     var_az  \n",
       "0  0.081890  \n",
       "0  0.040548  \n",
       "0  0.049907  \n",
       "0  0.073531  \n",
       "0  0.118898  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3480, 30)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>dz</th>\n",
       "      <th>fft_ax</th>\n",
       "      <th>fft_ay</th>\n",
       "      <th>fft_az</th>\n",
       "      <th>max_ax</th>\n",
       "      <th>max_ay</th>\n",
       "      <th>max_az</th>\n",
       "      <th>mean_ax</th>\n",
       "      <th>...</th>\n",
       "      <th>sd_az</th>\n",
       "      <th>sp_ax</th>\n",
       "      <th>sp_ay</th>\n",
       "      <th>sp_az</th>\n",
       "      <th>var_ax</th>\n",
       "      <th>var_ay</th>\n",
       "      <th>var_az</th>\n",
       "      <th>ts_start</th>\n",
       "      <th>ts_end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009452</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152901</td>\n",
       "      <td>-0.226779</td>\n",
       "      <td>10.250119</td>\n",
       "      <td>-0.333256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286165</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>0.058883</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.081890</td>\n",
       "      <td>152178985702</td>\n",
       "      <td>152178985912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023832</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.016422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.041873</td>\n",
       "      <td>-0.205003</td>\n",
       "      <td>10.182401</td>\n",
       "      <td>-0.422345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201366</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>0.064644</td>\n",
       "      <td>0.025264</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>152178985912</td>\n",
       "      <td>152178986124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.014844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383211</td>\n",
       "      <td>0.875952</td>\n",
       "      <td>10.347148</td>\n",
       "      <td>-0.312917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>0.077985</td>\n",
       "      <td>0.264340</td>\n",
       "      <td>0.049907</td>\n",
       "      <td>152178986124</td>\n",
       "      <td>152178986337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015739</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085185</td>\n",
       "      <td>1.039981</td>\n",
       "      <td>10.145551</td>\n",
       "      <td>-0.374978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271166</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>0.055386</td>\n",
       "      <td>0.045561</td>\n",
       "      <td>0.073531</td>\n",
       "      <td>152178986337</td>\n",
       "      <td>152178986549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>0.036462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.108875</td>\n",
       "      <td>0.941278</td>\n",
       "      <td>10.244975</td>\n",
       "      <td>-0.313581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344815</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>2.315844e+22</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.074459</td>\n",
       "      <td>0.118898</td>\n",
       "      <td>152178986549</td>\n",
       "      <td>152178986762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dx        dy        dz  fft_ax  fft_ay  fft_az    max_ax    max_ay  \\\n",
       "0  0.009452  0.010545  0.018279     0.0     0.0     0.0  0.152901 -0.226779   \n",
       "0  0.023832  0.009768  0.016422     0.0     0.0     0.0 -0.041873 -0.205003   \n",
       "0  0.012127  0.025754  0.014844     0.0     0.0     0.0  0.383211  0.875952   \n",
       "0  0.015739  0.013850  0.008753     0.0     0.0     0.0  0.085185  1.039981   \n",
       "0  0.003754  0.013616  0.036462     0.0     0.0     0.0 -0.108875  0.941278   \n",
       "\n",
       "      max_az   mean_ax  ...       sd_az         sp_ax         sp_ay  \\\n",
       "0  10.250119 -0.333256  ...    0.286165  2.315844e+22  2.315844e+22   \n",
       "0  10.182401 -0.422345  ...    0.201366  2.315844e+22  2.315844e+22   \n",
       "0  10.347148 -0.312917  ...    0.223400  2.315844e+22  2.315844e+22   \n",
       "0  10.145551 -0.374978  ...    0.271166  2.315844e+22  2.315844e+22   \n",
       "0  10.244975 -0.313581  ...    0.344815  2.315844e+22  2.315844e+22   \n",
       "\n",
       "          sp_az    var_ax    var_ay    var_az      ts_start        ts_end  \\\n",
       "0  2.315844e+22  0.058883  0.026466  0.081890  152178985702  152178985912   \n",
       "0  2.315844e+22  0.064644  0.025264  0.040548  152178985912  152178986124   \n",
       "0  2.315844e+22  0.077985  0.264340  0.049907  152178986124  152178986337   \n",
       "0  2.315844e+22  0.055386  0.045561  0.073531  152178986337  152178986549   \n",
       "0  2.315844e+22  0.023541  0.074459  0.118898  152178986549  152178986762   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "0      0  \n",
       "0      0  \n",
       "0      0  \n",
       "0      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting time stamps at the end\n",
    "cols = list(df_main.columns.values) #Make a list of all of the columns in the df\n",
    "cols.pop(cols.index('ts_start')) #Remove b from list\n",
    "cols.pop(cols.index('ts_end')) #Remove x from list\n",
    "cols.pop(cols.index('label')) # remove label\n",
    "df_main = df_main[cols+['ts_start','ts_end', 'label']]\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "df_main['fft_ax'] = preprocessing.scale(df_main['fft_ax'])\n",
    "df_main['fft_ay'] = preprocessing.scale(df_main['fft_ay'])\n",
    "df_main['fft_az'] = preprocessing.scale(df_main['fft_az'])\n",
    "\n",
    "df_main['sp_ax'] = preprocessing.scale(df_main['sp_ax'])\n",
    "df_main['sp_ay'] = preprocessing.scale(df_main['sp_ay'])\n",
    "df_main['sp_az'] = preprocessing.scale(df_main['sp_az'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3155\n",
       "1     325\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_main.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array(df_main)\n",
    "\n",
    "x = data[:,0:-3]\n",
    "y = data[:,-1:]\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3480, 27)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.81609195402298\n",
      "(2784, 27)   (696, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#acc_sum = 0\n",
    "#for i in range(100):\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_pred,y_test) * 100\n",
    "#acc_sum = acc_sum + accuracy\n",
    "print(accuracy)\n",
    "#acc_sum/100\n",
    "print(x_train.shape, \" \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[622,  10],\n",
       "       [ 40,  24]], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9238505747126436"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(kernel='rbf', C = 10)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[622,  10],\n",
       "       [ 43,  21]], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.99, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_trans_train = pca.transform(x_train)\n",
    "x_trans_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2784, 8)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trans_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9267241379310345"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC()\n",
    "model.fit(x_trans_train,y_train)\n",
    "y_pred = model.predict(x_trans_test)\n",
    "\n",
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[620,  12],\n",
       "       [ 39,  25]], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2331 samples, validate on 1149 samples\n",
      "Epoch 1/100\n",
      "2331/2331 [==============================] - 1s 531us/step - loss: 0.4543 - acc: 0.8224 - val_loss: 0.2236 - val_acc: 0.9104\n",
      "Epoch 2/100\n",
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.2863 - acc: 0.8996 - val_loss: 0.1961 - val_acc: 0.9260\n",
      "Epoch 3/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.2472 - acc: 0.9112 - val_loss: 0.1874 - val_acc: 0.9295\n",
      "Epoch 4/100\n",
      "2331/2331 [==============================] - 0s 116us/step - loss: 0.2378 - acc: 0.9138 - val_loss: 0.2005 - val_acc: 0.9304\n",
      "Epoch 5/100\n",
      "2331/2331 [==============================] - 0s 118us/step - loss: 0.2301 - acc: 0.9159 - val_loss: 0.1934 - val_acc: 0.9269\n",
      "Epoch 6/100\n",
      "2331/2331 [==============================] - 0s 125us/step - loss: 0.2302 - acc: 0.9151 - val_loss: 0.1841 - val_acc: 0.9304\n",
      "Epoch 7/100\n",
      "2331/2331 [==============================] - 0s 123us/step - loss: 0.2372 - acc: 0.9159 - val_loss: 0.1857 - val_acc: 0.9286\n",
      "Epoch 8/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.2188 - acc: 0.9121 - val_loss: 0.1861 - val_acc: 0.9252\n",
      "Epoch 9/100\n",
      "2331/2331 [==============================] - 0s 118us/step - loss: 0.2188 - acc: 0.9202 - val_loss: 0.1842 - val_acc: 0.9321\n",
      "Epoch 10/100\n",
      "2331/2331 [==============================] - 0s 122us/step - loss: 0.2090 - acc: 0.9198 - val_loss: 0.1804 - val_acc: 0.9295\n",
      "Epoch 11/100\n",
      "2331/2331 [==============================] - 0s 125us/step - loss: 0.2180 - acc: 0.9198 - val_loss: 0.1843 - val_acc: 0.9304\n",
      "Epoch 12/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.2089 - acc: 0.9198 - val_loss: 0.1838 - val_acc: 0.9330\n",
      "Epoch 13/100\n",
      "2331/2331 [==============================] - 0s 120us/step - loss: 0.2100 - acc: 0.9249 - val_loss: 0.1843 - val_acc: 0.9304\n",
      "Epoch 14/100\n",
      "2331/2331 [==============================] - 0s 126us/step - loss: 0.2093 - acc: 0.9185 - val_loss: 0.1844 - val_acc: 0.9304\n",
      "Epoch 15/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.2108 - acc: 0.9232 - val_loss: 0.1822 - val_acc: 0.9286\n",
      "Epoch 16/100\n",
      "2331/2331 [==============================] - 0s 124us/step - loss: 0.2086 - acc: 0.9155 - val_loss: 0.1831 - val_acc: 0.9321\n",
      "Epoch 17/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.2062 - acc: 0.9202 - val_loss: 0.1833 - val_acc: 0.9312\n",
      "Epoch 18/100\n",
      "2331/2331 [==============================] - 0s 125us/step - loss: 0.2049 - acc: 0.9245 - val_loss: 0.1834 - val_acc: 0.9339\n",
      "Epoch 19/100\n",
      "2331/2331 [==============================] - 0s 125us/step - loss: 0.2007 - acc: 0.9198 - val_loss: 0.1882 - val_acc: 0.9278\n",
      "Epoch 20/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.2107 - acc: 0.9206 - val_loss: 0.1838 - val_acc: 0.9304\n",
      "Epoch 21/100\n",
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.2070 - acc: 0.9241 - val_loss: 0.1827 - val_acc: 0.9321\n",
      "Epoch 22/100\n",
      "2331/2331 [==============================] - 0s 123us/step - loss: 0.2069 - acc: 0.9224 - val_loss: 0.1816 - val_acc: 0.9321\n",
      "Epoch 23/100\n",
      "2331/2331 [==============================] - 0s 123us/step - loss: 0.2039 - acc: 0.9236 - val_loss: 0.1810 - val_acc: 0.9347\n",
      "Epoch 24/100\n",
      "2331/2331 [==============================] - 0s 118us/step - loss: 0.2021 - acc: 0.9254 - val_loss: 0.1816 - val_acc: 0.9321\n",
      "Epoch 25/100\n",
      "2331/2331 [==============================] - 0s 125us/step - loss: 0.1992 - acc: 0.9236 - val_loss: 0.1797 - val_acc: 0.9295\n",
      "Epoch 26/100\n",
      "2331/2331 [==============================] - 0s 120us/step - loss: 0.2109 - acc: 0.9219 - val_loss: 0.1862 - val_acc: 0.9347\n",
      "Epoch 27/100\n",
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.2038 - acc: 0.9271 - val_loss: 0.1811 - val_acc: 0.9330\n",
      "Epoch 28/100\n",
      "2331/2331 [==============================] - 0s 122us/step - loss: 0.2074 - acc: 0.9228 - val_loss: 0.1790 - val_acc: 0.9295\n",
      "Epoch 29/100\n",
      "2331/2331 [==============================] - 0s 122us/step - loss: 0.1975 - acc: 0.9228 - val_loss: 0.1782 - val_acc: 0.9312\n",
      "Epoch 30/100\n",
      "2331/2331 [==============================] - 0s 123us/step - loss: 0.2050 - acc: 0.9254 - val_loss: 0.1836 - val_acc: 0.9286\n",
      "Epoch 31/100\n",
      "2331/2331 [==============================] - 0s 118us/step - loss: 0.2076 - acc: 0.9219 - val_loss: 0.1823 - val_acc: 0.9312\n",
      "Epoch 32/100\n",
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.2052 - acc: 0.9236 - val_loss: 0.1806 - val_acc: 0.9312\n",
      "Epoch 33/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.2038 - acc: 0.9279 - val_loss: 0.1799 - val_acc: 0.9356\n",
      "Epoch 34/100\n",
      "2331/2331 [==============================] - 0s 122us/step - loss: 0.2050 - acc: 0.9249 - val_loss: 0.1786 - val_acc: 0.9330\n",
      "Epoch 35/100\n",
      "2331/2331 [==============================] - 0s 120us/step - loss: 0.2092 - acc: 0.9189 - val_loss: 0.1808 - val_acc: 0.9339\n",
      "Epoch 36/100\n",
      "2331/2331 [==============================] - 0s 123us/step - loss: 0.2032 - acc: 0.9249 - val_loss: 0.1788 - val_acc: 0.9347\n",
      "Epoch 37/100\n",
      "2331/2331 [==============================] - 0s 123us/step - loss: 0.2060 - acc: 0.9249 - val_loss: 0.1799 - val_acc: 0.9347\n",
      "Epoch 38/100\n",
      "2331/2331 [==============================] - 0s 126us/step - loss: 0.1977 - acc: 0.9296 - val_loss: 0.1783 - val_acc: 0.9339\n",
      "Epoch 39/100\n",
      "2331/2331 [==============================] - 0s 123us/step - loss: 0.2034 - acc: 0.9219 - val_loss: 0.1812 - val_acc: 0.9286\n",
      "Epoch 40/100\n",
      "2331/2331 [==============================] - 0s 124us/step - loss: 0.1965 - acc: 0.9241 - val_loss: 0.1776 - val_acc: 0.9312\n",
      "Epoch 41/100\n",
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.2020 - acc: 0.9266 - val_loss: 0.1765 - val_acc: 0.9286\n",
      "Epoch 42/100\n",
      "2331/2331 [==============================] - 0s 122us/step - loss: 0.2037 - acc: 0.9314 - val_loss: 0.1780 - val_acc: 0.9295\n",
      "Epoch 43/100\n",
      "2331/2331 [==============================] - 0s 127us/step - loss: 0.2019 - acc: 0.9245 - val_loss: 0.1765 - val_acc: 0.9321\n",
      "Epoch 44/100\n",
      "2331/2331 [==============================] - 0s 124us/step - loss: 0.1948 - acc: 0.9288 - val_loss: 0.1776 - val_acc: 0.9312\n",
      "Epoch 45/100\n",
      "2331/2331 [==============================] - 0s 116us/step - loss: 0.1978 - acc: 0.9228 - val_loss: 0.1768 - val_acc: 0.9295\n",
      "Epoch 46/100\n",
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.1971 - acc: 0.9271 - val_loss: 0.1771 - val_acc: 0.9312\n",
      "Epoch 47/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.2016 - acc: 0.9258 - val_loss: 0.1825 - val_acc: 0.9304\n",
      "Epoch 48/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.1908 - acc: 0.9301 - val_loss: 0.1764 - val_acc: 0.9382\n",
      "Epoch 49/100\n",
      "2331/2331 [==============================] - 0s 124us/step - loss: 0.2000 - acc: 0.9262 - val_loss: 0.1745 - val_acc: 0.9347\n",
      "Epoch 50/100\n",
      "2331/2331 [==============================] - 0s 118us/step - loss: 0.2038 - acc: 0.9241 - val_loss: 0.1797 - val_acc: 0.9295\n",
      "Epoch 51/100\n",
      "2331/2331 [==============================] - 0s 124us/step - loss: 0.2005 - acc: 0.9241 - val_loss: 0.1782 - val_acc: 0.9312\n",
      "Epoch 52/100\n",
      "2331/2331 [==============================] - 0s 122us/step - loss: 0.2017 - acc: 0.9275 - val_loss: 0.1796 - val_acc: 0.9321\n",
      "Epoch 53/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.2019 - acc: 0.9232 - val_loss: 0.1806 - val_acc: 0.9312\n",
      "Epoch 54/100\n",
      "2331/2331 [==============================] - 0s 118us/step - loss: 0.1907 - acc: 0.9266 - val_loss: 0.1770 - val_acc: 0.9312\n",
      "Epoch 55/100\n",
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.1961 - acc: 0.9292 - val_loss: 0.1769 - val_acc: 0.9312\n",
      "Epoch 56/100\n",
      "2331/2331 [==============================] - 0s 123us/step - loss: 0.1990 - acc: 0.9271 - val_loss: 0.1784 - val_acc: 0.9312\n",
      "Epoch 57/100\n",
      "2331/2331 [==============================] - 0s 126us/step - loss: 0.1997 - acc: 0.9284 - val_loss: 0.1777 - val_acc: 0.9312\n",
      "Epoch 58/100\n",
      "2331/2331 [==============================] - 0s 128us/step - loss: 0.1929 - acc: 0.9305 - val_loss: 0.1751 - val_acc: 0.9356\n",
      "Epoch 59/100\n",
      "2331/2331 [==============================] - 0s 122us/step - loss: 0.1973 - acc: 0.9292 - val_loss: 0.1765 - val_acc: 0.9312\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.2019 - acc: 0.9254 - val_loss: 0.1781 - val_acc: 0.9312\n",
      "Epoch 61/100\n",
      "2331/2331 [==============================] - 0s 138us/step - loss: 0.1933 - acc: 0.9322 - val_loss: 0.1777 - val_acc: 0.9312\n",
      "Epoch 62/100\n",
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.2011 - acc: 0.9301 - val_loss: 0.1748 - val_acc: 0.9330\n",
      "Epoch 63/100\n",
      "2331/2331 [==============================] - 0s 118us/step - loss: 0.1962 - acc: 0.9266 - val_loss: 0.1799 - val_acc: 0.9312\n",
      "Epoch 64/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.1957 - acc: 0.9288 - val_loss: 0.1749 - val_acc: 0.9304\n",
      "Epoch 65/100\n",
      "2331/2331 [==============================] - 0s 127us/step - loss: 0.1928 - acc: 0.9266 - val_loss: 0.1724 - val_acc: 0.9321\n",
      "Epoch 66/100\n",
      "2331/2331 [==============================] - 0s 119us/step - loss: 0.1927 - acc: 0.9296 - val_loss: 0.1785 - val_acc: 0.9312\n",
      "Epoch 67/100\n",
      "2331/2331 [==============================] - 0s 159us/step - loss: 0.1959 - acc: 0.9318 - val_loss: 0.1800 - val_acc: 0.9295\n",
      "Epoch 68/100\n",
      "2331/2331 [==============================] - 0s 181us/step - loss: 0.1912 - acc: 0.9288 - val_loss: 0.1744 - val_acc: 0.9330\n",
      "Epoch 69/100\n",
      "2331/2331 [==============================] - 0s 152us/step - loss: 0.1913 - acc: 0.9279 - val_loss: 0.1759 - val_acc: 0.9312\n",
      "Epoch 70/100\n",
      "2331/2331 [==============================] - 0s 115us/step - loss: 0.1952 - acc: 0.9288 - val_loss: 0.1803 - val_acc: 0.9304\n",
      "Epoch 71/100\n",
      "2331/2331 [==============================] - 0s 151us/step - loss: 0.1908 - acc: 0.9279 - val_loss: 0.1757 - val_acc: 0.9286\n",
      "Epoch 72/100\n",
      "2331/2331 [==============================] - 0s 153us/step - loss: 0.1921 - acc: 0.9279 - val_loss: 0.1763 - val_acc: 0.9304\n",
      "Epoch 73/100\n",
      "2331/2331 [==============================] - 0s 126us/step - loss: 0.1884 - acc: 0.9288 - val_loss: 0.1779 - val_acc: 0.9330\n",
      "Epoch 74/100\n",
      "2331/2331 [==============================] - 0s 146us/step - loss: 0.1979 - acc: 0.9275 - val_loss: 0.1798 - val_acc: 0.9295\n",
      "Epoch 75/100\n",
      "2331/2331 [==============================] - 0s 145us/step - loss: 0.1865 - acc: 0.9279 - val_loss: 0.1775 - val_acc: 0.9312\n",
      "Epoch 76/100\n",
      "2331/2331 [==============================] - 0s 135us/step - loss: 0.1933 - acc: 0.9236 - val_loss: 0.1753 - val_acc: 0.9330\n",
      "Epoch 77/100\n",
      "2331/2331 [==============================] - 0s 160us/step - loss: 0.1962 - acc: 0.9262 - val_loss: 0.1790 - val_acc: 0.9312\n",
      "Epoch 78/100\n",
      "2331/2331 [==============================] - 0s 158us/step - loss: 0.1963 - acc: 0.9288 - val_loss: 0.1763 - val_acc: 0.9330\n",
      "Epoch 79/100\n",
      "2331/2331 [==============================] - 0s 162us/step - loss: 0.1886 - acc: 0.9266 - val_loss: 0.1763 - val_acc: 0.9330\n",
      "Epoch 80/100\n",
      "2331/2331 [==============================] - 0s 155us/step - loss: 0.1913 - acc: 0.9258 - val_loss: 0.1785 - val_acc: 0.9356\n",
      "Epoch 81/100\n",
      "2331/2331 [==============================] - 0s 160us/step - loss: 0.1984 - acc: 0.9245 - val_loss: 0.1747 - val_acc: 0.9312\n",
      "Epoch 82/100\n",
      "2331/2331 [==============================] - 0s 162us/step - loss: 0.1923 - acc: 0.9309 - val_loss: 0.1740 - val_acc: 0.9339\n",
      "Epoch 83/100\n",
      "2331/2331 [==============================] - 0s 152us/step - loss: 0.1915 - acc: 0.9266 - val_loss: 0.1761 - val_acc: 0.9304\n",
      "Epoch 84/100\n",
      "2331/2331 [==============================] - 0s 159us/step - loss: 0.1921 - acc: 0.9271 - val_loss: 0.1742 - val_acc: 0.9330\n",
      "Epoch 85/100\n",
      "2331/2331 [==============================] - 0s 123us/step - loss: 0.1881 - acc: 0.9301 - val_loss: 0.1727 - val_acc: 0.9339\n",
      "Epoch 86/100\n",
      "2331/2331 [==============================] - 0s 131us/step - loss: 0.1941 - acc: 0.9241 - val_loss: 0.1757 - val_acc: 0.9295\n",
      "Epoch 87/100\n",
      "2331/2331 [==============================] - 0s 130us/step - loss: 0.1886 - acc: 0.9296 - val_loss: 0.1763 - val_acc: 0.9304\n",
      "Epoch 88/100\n",
      "2331/2331 [==============================] - 0s 125us/step - loss: 0.1855 - acc: 0.9309 - val_loss: 0.1743 - val_acc: 0.9339\n",
      "Epoch 89/100\n",
      "2331/2331 [==============================] - 0s 161us/step - loss: 0.1874 - acc: 0.9275 - val_loss: 0.1786 - val_acc: 0.9304\n",
      "Epoch 90/100\n",
      "2331/2331 [==============================] - 0s 156us/step - loss: 0.1946 - acc: 0.9249 - val_loss: 0.1765 - val_acc: 0.9356\n",
      "Epoch 91/100\n",
      "2331/2331 [==============================] - 0s 162us/step - loss: 0.1875 - acc: 0.9301 - val_loss: 0.1770 - val_acc: 0.9312\n",
      "Epoch 92/100\n",
      "2331/2331 [==============================] - 0s 120us/step - loss: 0.1880 - acc: 0.9314 - val_loss: 0.1742 - val_acc: 0.9365\n",
      "Epoch 93/100\n",
      "2331/2331 [==============================] - 0s 136us/step - loss: 0.1955 - acc: 0.9249 - val_loss: 0.1755 - val_acc: 0.9347\n",
      "Epoch 94/100\n",
      "2331/2331 [==============================] - 0s 162us/step - loss: 0.1909 - acc: 0.9275 - val_loss: 0.1718 - val_acc: 0.9347\n",
      "Epoch 95/100\n",
      "2331/2331 [==============================] - 0s 165us/step - loss: 0.1915 - acc: 0.9249 - val_loss: 0.1727 - val_acc: 0.9373\n",
      "Epoch 96/100\n",
      "2331/2331 [==============================] - 0s 147us/step - loss: 0.1898 - acc: 0.9309 - val_loss: 0.1729 - val_acc: 0.9356\n",
      "Epoch 97/100\n",
      "2331/2331 [==============================] - 0s 124us/step - loss: 0.1835 - acc: 0.9288 - val_loss: 0.1755 - val_acc: 0.9339\n",
      "Epoch 98/100\n",
      "2331/2331 [==============================] - 0s 121us/step - loss: 0.1928 - acc: 0.9296 - val_loss: 0.1736 - val_acc: 0.9373\n",
      "Epoch 99/100\n",
      "2331/2331 [==============================] - 0s 135us/step - loss: 0.1894 - acc: 0.9258 - val_loss: 0.1692 - val_acc: 0.9339\n",
      "Epoch 100/100\n",
      "2331/2331 [==============================] - 0s 158us/step - loss: 0.1907 - acc: 0.9284 - val_loss: 0.1713 - val_acc: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9329852045256745"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment following to run neural net\n",
    "\n",
    "#acc = []\n",
    "#for i in range(50):\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.33)\n",
    "# model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100,activation=\"relu\",input_dim =x_train.shape[1]))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=50,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=10,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1,activation=\"sigmoid\"))\n",
    "#model.add(Dense(units=10,activation=\"softmax\"))\n",
    "\n",
    "#compile\n",
    "model.compile(optimizer='adam',loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "#train\n",
    "model.fit(x_train,y_train, validation_data= (x_test, y_test), batch_size= 50, epochs= 100)\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.924978975849082"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAHCCAYAAADl4BDwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c590c57208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm           =cnf_mat, \n",
    "                      normalize    = True,\n",
    "                      target_names = ['non-potholes', 'potholes'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements, counts =  np.unique(y_test, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    1]\n",
      " [1041  108]]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray((elements, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
